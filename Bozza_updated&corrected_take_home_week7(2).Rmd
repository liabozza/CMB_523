---
title: "Machine Learning to Classify Breast Tissue Leisions as Benign or Malignant"
output: html_notebook
author: Lia Bozza and Prof Fisk
---

Today, we are going to work with some real data and machine to get used to the process of using packages without *really* knowing what the package is doing. 


UC Irvine provides a lot of data for oncology machine learning training. 
You are going to write a random forest classifier that will learn how to classify breast leisions as benign or malignant.

I've labelled the columns of the data for you. There are 32 columns, 30 of which contain data that we are going to use to train our model. One of the remaining columns (the first) is an ID that identifies the sample uniquely and the other (the second) contains the answer (known as the label) that we are going to train our machine learning algorithm to classify. 

We are going to use the caret and randomForest packages to do so.
 
```{r}
if(!"caret"%in%installed.packages()){install.packages("caret", dependencies = TRUE)}
if(!"randomForest"%in%installed.packages()){install.packages("randomForest")}
library(caret)
library(randomForest)
```

Next, lets read in the data.

```{r}
#set FILENAME to be the location of the file on your computer
FILENAME<-"C:/Users/masoo/Downloads/breastFNA.csv"  
all_breast_data<-read.csv(FILENAME,header = T)
```

The above code should load the data in as a variable called "all_breast_data". One common feature of machine learning classifiers is that we need to split the data we have into different pieces called training and testing. The training data is what the computer will use to analyze underlying and complex relationships between the data and the labels. The testing data is what we will use to determine if the machine learning algorithm worked well or not (that is, we leave some data out so we can give the program a test it doesn't know the answer to). 

```{r}
print(nrow(all_breast_data))#<write a line of code that prints how many rows are in the data>
```

It is not uncommon to set aside around one-fifth of your data for testing. To make things easy to interpret, you will set aside 100 samples for testing. Split all_breast_data into two variables, one called testDat, containing 100 random rows from all_breast_data, and trainDat, containing the rest.

```{r}
set.seed(42666) #just makes it easier for Dr. Fisk to grade

#YOUR CODE HERE
all_breast_data_rows<-c(1:nrow(all_breast_data))
#sample((1:all_breast_data_rows), replace = FALSE) no need for this, look down
vector_of_numbers_rows<-sample(1:569,size = 100, replace = FALSE)
tesDat<-all_breast_data[vector_of_numbers_rows,] 
trainDat<-all_breast_data[-vector_of_numbers_rows,]
print(trainDat)
#print the rows
#there are lots of ways to do this
#if you are having a hard time figuring out what to do
#then I recommend creating a vector of all the numbers between 1
#and the number of rows. Then, I would use the sample() function without replacement to randomly pick 100 row numbers from the aforementioned vector and save it as a new vector.
#Then, test would be testDat<-all_breast_data[new_vector,] and train would be trainDat<-all_breast_data[-new_vector,]
#however you do it is fine

```
Great! Now that you've split your data in two, we are going to train our machine learning model! We need to do just a bit of data cleaning first by converting our training label to something called a factor.

QUESTION 1: What is a factor data type in R?
<Using a string to record a lot of variables can lead to typos, but if you group big amounts of variables in one variable that represents them, you can split big data in useful ways. One example is splitting the months of the year by season, instead of individually combining 4 months every time you want to use that data.> 

```{r}
##NONE OF THIS IS CODE YOU NEED TO CHANGE OR ADD TO, BUT LOOK IT OVER ALL THE SAME

#diagnosis is what we are trying to predict
#B (benign) or M (malignant)
trainDat$Diagnosis<-factor(trainDat$Diagnosis)
#now we are going to train the model
#we are using the randomforest algorithm, which is 'rf' and what is called a 5 fold cross-validation where the algorithm is trained on random subsets of the data and tested to get initial accuracy. 
model<-train(Diagnosis~.,
            data=trainDat,
            method ='rf',
            trControl=trainControl(method = "cv",number = 5)
)

print(model)
```

QUESTION 2: What mtry value had the best accuracy for your model?
mtry 31 has the best accuracy for this model

mtry is what is called a hyperparameter. It is basically a number that somehow shapes how well the machine learning model does. For random forests, they have a hyperparameter called mtry that decides how many variables are used at each split in the decision tree. 


Now we are going to test our model on data it has never seen before (trainDat) and see how well it learned.
```{r}
###AGAIN, NO NEED TO ALTER ANY OF THIS RIGHT NOW

#need to convert the diagnosis into a factor in the test data too
tesDat$Diagnosis<-factor(tesDat$Diagnosis)

#make the predictions and store them in a new column called prediction
tesDat$Prediction<-predict(model, newdata = tesDat)

```

Now your variable trainDat has a new column with the predictions the machine model made, given the other columns. How did it do? You are going to write code to find out! The "Diagnosis" column has the true answers. The "Prediction" column has the predicted answers. Your task here is to determine for how many of the rows out of 100 they do not match.

```{r}
#YOU DO NEED CODE HERE
#write code to determine how many values in trainDat's Prediction column (accessible with $) do not match the value in it's Diagnosis column (accessible with $)
#trainDat$Prediction[!(trainDat$Prediction %in% trainDat$Diagnosis)] this = "is there any "B" or "M" in the data, not how many match...corrected below Dr. Fisk suggestion
sum(tesDat$Prediction!=tesDat$Diagonsis) 
#setdiff(trainDat$Prediction, trainDat$Diagnosis) want to learn this. will test out with small data later
```

Below is code that will tell you the true diagnosis of those that the model misclassified. That is, when it was wrong, what was the right answer.
```{r}
#NO NEED FOR YOU TO ADD CODE HERE
print(summary(tesDat[which(tesDat$Diagnosis!=tesDat$Prediction),"Diagnosis"]))
```
QUESTION 3: In your results, was there a bias to which kind of leision was misclassified more often? 
The results show no bias to misclassification. It seems that benign and malignant tumors could be misclassified equally or pathologists share tissue sections with other hospitals when they don't know how to classify a specimen.

QUESTION 4: Do you think that it would be better for the machine learning algorithm to classify benign tumors as malignant or malignant tumors as benign. What other information might you want to make that decision? (Not really a right or wrong questions, but making you think about what your data looks like)
It is better for the algorithm to classify benign tumors as malignant because malignant is worst, have to assume possibility of the worst scenario until you can disprove it. You cannot assume the best and hope it doesn't get worse. 

There are only 2 outcomes the model could decide: B or M.

QUESTION 5: If you or your model just guessed at random (equal chance in this case), what do you expect your accuracy to be?
Even though B or M have equal chance of classification, if the model guessed at random, the accuracy could be even less than 50% because it could misclassify both B and M samples over the given total sample data.

There might be more to it than you thought at first glance. Lets think back to all the data. How many samples in the training data are B and how many are M?
```{r}
#NO NEED TO ADD CODE HERE
print(table(all_breast_data$Diagnosis))
print(table(all_breast_data$Diagnosis)/nrow(all_breast_data))
```
Hmm... around 63% of the data is benign. That means that if you guessed benign every time, no matter what, you would get an accuracy of 63%! Better than the (probably) 50% you guessed above. Indeed, guessing at random (with equal probabilities, like a coin flip) would get you below a 50% accuracy on this data, on average because there is more benign data than malignant data.

Why is this important? In your projects (and in your life!) understanding what we expect to see is key to understanding whether or not what we see is suprising or important. In this case, the dumbest model could achieve 63% accuracy by just always guessing benign. So if you wrote a program that gave you an accuracy of, say, 70%, that would not actually be a very impressive machine learning program!

Think critically about your data whenever you can. Ask "are the characteristics uniformly distributed among members of my data?" or more simply "what do I expect to see and how often do I expect to see it". 


We are going to see if doing any preprocessing of our data helps with the outcome at all. The chances are it probably won't for a random forest, but it is good practice all the same. All the new bit of code below does is change the numbers such that they are centered about a single number and scaled proportionally between 0 and 1. There are reasons this can help not worth getting into today.

```{r}
#NO NEED FOR YOU TO ALTER THIS CODE
#Same thing but with preprocessing
#You can use this block of code again down below.
model2<-train(Diagnosis~.,
            data=trainDat,
            method ='rf',
            preProc=c("center","scale"), #note this line
            trControl=trainControl(method = "cv",number = 5)
)

trainDat$Prediction2<-predict(model2, newdata = trainDat)
print(summary(trainDat[which(trainDat$Diagnosis!=trainDat$Prediction2),"Diagnosis"]))

```
As expected, the preprocessing didn't help in this case. However, more numerically intensive methods can benefit from this preprocessing. 

Here is where I leave you with one last counter-intutive fact about working with data. 

More isn't always better.

```{r}
# NO NEED FOR YOU TO ALTER THE CODE HERE
#plot the model
ggplot(model2)
```

As the decision tree considers more and more predictors to include (that is, columns to include), its prediction accuracy actually goes down. Collecting more rows of data can usually help this problem, but in biology we are often dealing with lots of columns, few rows. For instance, each location in the human genome could be considered a column. 

A fundamental task and skill of scientific computing and the analysis of biological data is using your biological knowledge and your tech skills to determine what to include and what to exclude from your data in a way that doesn't also just bias the answer to what you want to see.

A delicate balance indeed!


You last task for today is to repeat the process of training the data, predicting the diagnoses, and calcultaint the % correct and which ones were wrong using a different machine learning method.
[A list of available models compatable can be found here.](https://topepo.github.io/caret/available-models.html)
Remember that you are performing a classification task, so don't use a regression model. You may need to install another packackage(s) to get the code to work. But ultimately, your should really have to change the method="rf" parameter from the train function above. Plot the model once it is done training, like I did above with the ggplot function. 


```{r}
#if(!"earth"%in%installed.packages()){install.packages("earth", dependencies = TRUE)}
#if(!"mda"%in%installed.packages()){install.packages("mda")}
#library(earth)
#library(mda) #(These are libraries needed for Bagged Flexible Discriminant Analysis Model)
if(!"rFerns"%in%installed.packages()){install.packages("rFerns", dependencies = TRUE)}
#if(!"train"%in%installed.packages()){install.packages("train", dependencies = TRUE)}
library(rFerns)
#if(!"plyr"%in%installed.packages()){install.packages("plyr", dependencies = TRUE)}
#library(plyr)
FILENAME<-"C:/Users/masoo/Downloads/breastFNA.csv"  
all_breast_data<-read.csv(FILENAME,header = T)
print(nrow(all_breast_data))
set.seed(42666) #Dr. Fisk uses this to grade

all_breast_data_rows<-c(1:nrow(all_breast_data))
#sample((1:all_breast_data_rows), replace = FALSE) no need for this, look down
vector_of_numbers_rows<-sample(1:569,size = 100, replace = FALSE)
tesDat<-all_breast_data[vector_of_numbers_rows,] 
trainDat<-all_breast_data[-vector_of_numbers_rows,]
print(trainDat)

trainDat$Diagnosis<-factor(trainDat$Diagnosis) #makes diagnosis part of train data
#make the predictions and store them in a new column called prediction
trainDat$Prediction<-predict(model, newdata = trainDat)

trainDat$Prediction[!(trainDat$Prediction %in% trainDat$Diagnosis)] #h this is how many values in trainDat's Prediction column do not match the value in it's Diagnosis column
  print(trainDat$Prediction[!(trainDat$Prediction %in% trainDat$Diagnosis)]) #print to check with above answer factor() levels BM, correct answer, check
  
print(table(all_breast_data$Diagnosis))
print(table(all_breast_data$Diagnosis)/nrow(all_breast_data))
  

#fitControl <- trainControl(method = "repeatedcv", # 10-fold CV, added this because error in ggplot; no tuning parameters with more than 1 value
 #                          number = 10,# repeated 10 times
  #                         repeats = 10)

model2<-train(Diagnosis~., #changing code to be centered around a single number
            data=trainDat,
            method ='rFerns', 
            preProc=c("center","scale"), #note this line
            trControl=trainControl(method = "cv",number = 5) 
)

trainDat$Prediction2<-predict(model2, newdata = trainDat)
print(summary(trainDat[which(trainDat$Diagnosis!=trainDat$Prediction2),"Diagnosis"]))


trainDat$Prediction2<-predict(model2, newdata = trainDat)
print(trainDat$Prediction2<-predict(model2, newdata = trainDat))
print(summary(trainDat[which(trainDat$Diagnosis!=trainDat$Prediction2),"Diagnosis"]))

if(!"ggplot2"%in%installed.packages()){install.packages("ggplot2", dependencies = TRUE)}
library(ggplot2)
print(!TRUE)
ggplot(model2)

### Your code here, including installing and loading (via library) whatever other packages you end up needing to use
### you can copy what I did above--that is ultimately how I wrote and ran the code, too!
### change the method from 'rf' to another classification method.
### don't forget to train the model first, then predict the outcomes, then calculate how many you got write, then plot the model.
### this should be mostly copy-ing and pasting, like you will be doing in your final projects!

```

QUESTION 6: What did you think of this assignment?
This assignment was difficult for me to think about because at a point, I was removed from the human behind the breast tumor data and I forgot that everyone's lives were changed who was documented here. It was difficult also because it took a long time to find a train models part of random forest that would work with ggplot. A few that I tried were removed from the CRAN repository.
